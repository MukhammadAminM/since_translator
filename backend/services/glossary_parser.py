"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –≥–ª–æ—Å—Å–∞—Ä–∏—è
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç—ã: TXT, PDF, DOCX
–° –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π OCR –¥–ª—è PDF —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
"""
import re
import json
import os
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# OCR –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
try:
    import pytesseract
    from PIL import Image
    from pdf2image import convert_from_path
    OCR_AVAILABLE = True
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–∏ –∫ Tesseract –¥–ª—è Windows (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
    if os.name == 'nt':  # Windows
        tesseract_paths = [
            r'C:\Program Files\Tesseract-OCR\tesseract.exe',
            r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe',
        ]
        for path in tesseract_paths:
            if Path(path).exists():
                pytesseract.pytesseract.tesseract_cmd = path
                break
except ImportError:
    OCR_AVAILABLE = False

# –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å PDF
try:
    import PyPDF2
    PDF_AVAILABLE = True
    PDF_LIB = "PyPDF2"
except ImportError:
    try:
        import pdfplumber
        PDF_AVAILABLE = True
        PDF_LIB = "pdfplumber"
    except ImportError:
        try:
            import fitz  # PyMuPDF
            PDF_AVAILABLE = True
            PDF_LIB = "pymupdf"
        except ImportError:
            PDF_AVAILABLE = False
            PDF_LIB = None

try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False

try:
    import openpyxl
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False


class GlossaryParser:
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤ –≥–ª–æ—Å—Å–∞—Ä–∏—è
    """
    
    def __init__(self, glossary_dir: str = "glossary"):
        self.glossary_dir = Path(glossary_dir)
    
    def parse_txt_file(self, file_path: Path) -> List[Tuple[str, str]]:
        """
        –ü–∞—Ä—Å–∏—Ç TXT —Ñ–∞–π–ª —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
        "–†—É—Å—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω ‚Äì English translation"
        –∏–ª–∏
        "1. –†—É—Å—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω ‚Äì English translation"
        """
        terms = []
        
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith("#"):
                    continue
                
                # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏
                line = re.sub(r'^\d+\.\s*', '', line)
                
                # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å "‚Äì" –∏–ª–∏ "-"
                if "‚Äì" in line:
                    parts = line.split("‚Äì", 1)
                elif " - " in line:
                    parts = line.split(" - ", 1)
                elif " ‚Äî " in line:
                    parts = line.split(" ‚Äî ", 1)
                else:
                    continue
                
                if len(parts) == 2:
                    source_term = self._normalize_text(parts[0].strip())
                    target_term = self._normalize_text(parts[1].strip())
                    
                    if source_term and target_term:
                        terms.append((source_term, target_term))
        
        return terms
    
    def parse_pdf_file(self, file_path: Path) -> List[Tuple[str, str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏ –ø–∞—Ä—Å–∏—Ç —Ç–µ—Ä–º–∏–Ω—ã
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç PyPDF2, pdfplumber –∏ PyMuPDF
        """
        if not PDF_AVAILABLE:
            raise ImportError(
                "PDF –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –æ–¥–Ω—É –∏–∑: "
                "pip install PyPDF2 –∏–ª–∏ pip install pdfplumber –∏–ª–∏ pip install pymupdf"
            )
        
        text = ""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
        if PDF_LIB == "PyPDF2":
            with open(file_path, "rb") as f:
                pdf_reader = PyPDF2.PdfReader(f)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
        elif PDF_LIB == "pdfplumber":
            import pdfplumber
            with pdfplumber.open(file_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
        elif PDF_LIB == "pymupdf":
            import fitz
            doc = fitz.open(file_path)
            for page in doc:
                text += page.get_text() + "\n"
            doc.close()
        
        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π, –ø—Ä–æ–±—É–µ–º OCR (PDF –º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
        if not text.strip() or len(text.strip()) < 50:
            if not text.strip():
                print(f"‚ö†Ô∏è  –¢–µ–∫—Å—Ç –Ω–µ –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ {file_path.name}, –ø—Ä–æ–±—É–µ–º OCR...")
            else:
                print(f"‚ö†Ô∏è  –ú–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ {file_path.name} ({len(text.strip())} —Å–∏–º–≤–æ–ª–æ–≤), –ø—Ä–æ–±—É–µ–º OCR...")
            ocr_text = self._extract_text_with_ocr(file_path)
            if ocr_text:
                text = ocr_text
        
        if not text.strip():
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {file_path.name}")
            return []
        
        # –ü–∞—Ä—Å–∏–º —Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ TXT
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∫ –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–π, —Ç–∞–∫ –∏ –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (–¥–ª—è –∞—Ä–∞–±—Å–∫–æ–≥–æ)
        terms = []
        lines = [line.strip() for line in text.split("\n")]
        
        i = 0
        while i < len(lines):
            line = lines[i]
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
            if not line:
                i += 1
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç—Ä–æ–∫—É –¥–ª—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
            original_line = line
            
            # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é
            line = re.sub(r'^\d+\.\s*', '', line)
            
            # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –Ω—É–º–µ—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∫–∞ –ø—É—Å—Ç–∞—è, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π
            if not line:
                i += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (—Ç–µ—Ä–º–∏–Ω ‚Äì –ø–µ—Ä–µ–≤–æ–¥)
            separators = ["‚Äì", "‚Äî", " - ", " -", "- ", ":", "=", "‚Üí"]
            parts = None
            
            for sep in separators:
                if sep in line:
                    parts = line.split(sep, 1)
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ —Å—Ç—Ä–æ–∫–µ, –ø—Ä–æ–±—É–µ–º regex
            if not parts or len(parts) != 2:
                match = re.search(r'([\u0600-\u06FF\s]+)[\s\-‚Äì‚Äî:=‚Üí]+([A-Za-z\s\(\)]+)', line)
                if match:
                    parts = [match.group(1).strip(), match.group(2).strip()]
                else:
                    match = re.search(r'([A-Za-z\s\(\)]+)[\s\-‚Äì‚Äî:=‚Üí]+([\u0600-\u06FF\s]+)', line)
                    if match:
                        parts = [match.group(2).strip(), match.group(1).strip()]
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –æ–¥–Ω–æ—Å—Ç—Ä–æ—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, –ø—Ä–æ–±—É–µ–º –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π
            # –§–æ—Ä–º–∞—Ç: –∞—Ä–∞–±—Å–∫–∏–π_—Ç–µ—Ä–º–∏–Ω (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–æ–∫–∞—Ö)\n‚Äì\n–∞–Ω–≥–ª–∏–π—Å–∫–∏–π_–ø–µ—Ä–µ–≤–æ–¥
            if (not parts or len(parts) != 2) and i + 1 < len(lines):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞—Ä–∞–±—Å–∫–∏–π —Ç–µ–∫—Å—Ç –≤ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–µ
                arabic_in_current = re.search(r'[\u0600-\u06FF]', line)
                
                if arabic_in_current:
                    # –°–æ–±–∏—Ä–∞–µ–º –∞—Ä–∞–±—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–æ–∫ (–¥–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è)
                    arabic_parts = [line]
                    j = i + 1
                    
                    # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                    while j < len(lines):
                        if not lines[j].strip():
                            j += 1
                            continue
                        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                        if lines[j].strip() in ["‚Äì", "‚Äî", "-"]:
                            separator_line = j
                            j += 1
                            break
                        # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ –Ω–æ–º–µ—Ä –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç, —ç—Ç–æ –Ω–µ –∞—Ä–∞–±—Å–∫–∏–π —Ç–µ—Ä–º–∏–Ω
                        if re.match(r'^\d+\.', lines[j]) or re.search(r'[A-Za-z]', lines[j]):
                            break
                        # –ï—Å–ª–∏ –µ—Å—Ç—å –∞—Ä–∞–±—Å–∫–∏–π —Ç–µ–∫—Å—Ç, –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Ç–µ—Ä–º–∏–Ω—É
                        if re.search(r'[\u0600-\u06FF]', lines[j]):
                            arabic_parts.append(lines[j])
                        j += 1
                    
                    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å, —Å–æ–±–∏—Ä–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥
                    if 'separator_line' in locals() and j < len(lines):
                        source_term = " ".join(arabic_parts)
                        target_parts = []
                        
                        # –°–æ–±–∏—Ä–∞–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –ø–µ—Ä–µ–≤–æ–¥ –ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
                        while j < len(lines):
                            if not lines[j].strip():
                                j += 1
                                continue
                            # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ –Ω–æ–≤—ã–π —Ç–µ—Ä–º–∏–Ω (–∞—Ä–∞–±—Å–∫–∏–π –∏–ª–∏ –Ω–æ–º–µ—Ä), –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è
                            if (re.search(r'[\u0600-\u06FF]', lines[j]) or 
                                re.match(r'^\d+\.', lines[j]) or
                                lines[j].strip() in ["‚Äì", "‚Äî", "-"]):
                                break
                            # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Ç–µ–∫—Å—Ç
                            if re.search(r'[A-Za-z]', lines[j]):
                                target_parts.append(lines[j].strip())
                            j += 1
                        
                        if target_parts:
                            target_term = " ".join(target_parts)
                            parts = [source_term, target_term]
                            i = j - 1  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            
            if parts and len(parts) == 2:
                source_term = parts[0].strip()
                target_term = parts[1].strip()
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –Ω–µ–ø—É—Å—Ç–æ–π —Ç–µ—Ä–º–∏–Ω
                if source_term and target_term:
                    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
                    source_term = re.sub(r'\s+', ' ', source_term)
                    target_term = re.sub(r'\s+', ' ', target_term)
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
                    source_term = self._normalize_text(source_term)
                    target_term = self._normalize_text(target_term)
                    terms.append((source_term, target_term))
            
            i += 1
        
        return terms
    
    def parse_docx_file(self, file_path: Path) -> List[Tuple[str, str]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ DOCX –∏ –ø–∞—Ä—Å–∏—Ç —Ç–µ—Ä–º–∏–Ω—ã
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∫ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã, —Ç–∞–∫ –∏ —Ç–∞–±–ª–∏—Ü—ã
        """
        if not DOCX_AVAILABLE:
            raise ImportError("python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install python-docx")
        
        doc = Document(file_path)
        terms = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        for paragraph in doc.paragraphs:
            line = paragraph.text.strip()
            if not line:
                continue
            
            # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é
            line = re.sub(r'^\d+\.\s*', '', line)
            
            # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
            if "‚Äì" in line:
                parts = line.split("‚Äì", 1)
            elif " - " in line:
                parts = line.split(" - ", 1)
            elif " ‚Äî " in line:
                parts = line.split(" ‚Äî ", 1)
            else:
                continue
            
            if len(parts) == 2:
                source_term = parts[0].strip()
                target_term = parts[1].strip()
                
                if source_term and target_term:
                    terms.append((source_term, target_term))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã (—Ñ–æ—Ä–º–∞—Ç: –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ | —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å | —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞)
        for table in doc.tables:
            for row in table.rows:
                cells = [cell.text.strip() for cell in row.cells]
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                if not any(cells) or len(cells) < 2:
                    continue
                
                # –§–æ—Ä–º–∞—Ç 1: [–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞, "-", –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞] (3 –∫–æ–ª–æ–Ω–∫–∏)
                if len(cells) >= 3:
                    abbrev = cells[0].strip()
                    separator = cells[1].strip()
                    expansion = cells[2].strip()
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ä–µ–¥–Ω—è—è –∫–æ–ª–æ–Ω–∫–∞ - —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                    if separator in ["-", "‚Äì", "‚Äî", ":", "="] and abbrev and expansion:
                        # –ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ -> –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞
                        abbrev = self._normalize_text(abbrev)
                        expansion = self._normalize_text(expansion)
                        terms.append((abbrev, expansion))
                        continue
                
                # –§–æ—Ä–º–∞—Ç 2: [–ê–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞, –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞] (2 –∫–æ–ª–æ–Ω–∫–∏)
                if len(cells) >= 2:
                    cell1 = cells[0].strip()
                    cell2 = cells[1].strip()
                    
                    # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ –∫–æ—Ä–æ—Ç–∫–∞—è (–≤–µ—Ä–æ—è—Ç–Ω–æ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞) –∏ –≤—Ç–æ—Ä–∞—è –¥–ª–∏–Ω–Ω–∞—è
                    if (len(cell1) <= 20 and len(cell2) > len(cell1) and 
                        cell1 and cell2 and 
                        not re.search(r'[‚Äì‚Äî]', cell1)):  # –ù–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è –≤ –ø–µ—Ä–≤–æ–π –∫–æ–ª–æ–Ω–∫–µ
                        cell1 = self._normalize_text(cell1)
                        cell2 = self._normalize_text(cell2)
                        terms.append((cell1, cell2))
                        continue
                
                # –§–æ—Ä–º–∞—Ç 3: [–¢–µ—Ä–º–∏–Ω - –ü–µ—Ä–µ–≤–æ–¥] –≤ –æ–¥–Ω–æ–π —è—á–µ–π–∫–µ
                for cell_text in cells:
                    if not cell_text:
                        continue
                    
                    # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ —è—á–µ–π–∫–µ
                    if "‚Äì" in cell_text:
                        parts = cell_text.split("‚Äì", 1)
                    elif " - " in cell_text:
                        parts = cell_text.split(" - ", 1)
                    elif " ‚Äî " in cell_text:
                        parts = cell_text.split(" ‚Äî ", 1)
                    else:
                        continue
                    
                    if len(parts) == 2:
                        source_term = self._normalize_text(parts[0].strip())
                        target_term = self._normalize_text(parts[1].strip())
                        
                        if source_term and target_term:
                            terms.append((source_term, target_term))
        
        return terms
    
    def _extract_text_with_ocr(self, file_path: Path) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF –∏—Å–ø–æ–ª—å–∑—É—è OCR (—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
        –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π Tesseract OCR –∏ poppler
        """
        if not OCR_AVAILABLE:
            print("‚ö†Ô∏è  OCR –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install pytesseract pdf2image Pillow")
            print("   –¢–∞–∫–∂–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Tesseract OCR: https://github.com/tesseract-ocr/tesseract")
            return ""
        
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            print(f"   –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
            
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ poppler –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            poppler_path = None
            if os.name == 'nt':  # Windows
                poppler_paths = [
                    r'C:\poppler\Library\bin',
                    r'C:\poppler\bin',
                    r'C:\Program Files\poppler\bin',
                ]
                for path in poppler_paths:
                    if Path(path).exists():
                        poppler_path = path
                        break
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º PDF –≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if poppler_path:
                images = convert_from_path(str(file_path), dpi=300, poppler_path=poppler_path)
            else:
                images = convert_from_path(str(file_path), dpi=300)
            
            text = ""
            print(f"   –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ {len(images)} —Å—Ç—Ä–∞–Ω–∏—Ü...")
            
            for i, image in enumerate(images, 1):
                print(f"   –°—Ç—Ä–∞–Ω–∏—Ü–∞ {i}/{len(images)}...", end="\r")
                # –†–∞—Å–ø–æ–∑–Ω–∞–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
                page_text = pytesseract.image_to_string(
                    image, 
                    lang='rus+eng',  # –†—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π
                    config='--psm 6'  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –µ–¥–∏–Ω—ã–π –±–ª–æ–∫ —Ç–µ–∫—Å—Ç–∞
                )
                text += page_text + "\n"
            
            print(f"   ‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return text
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ OCR: {str(e)}")
            print("   –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ Tesseract OCR —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –¥–æ—Å—Ç—É–ø–µ–Ω –≤ PATH")
            return ""
    
    def parse_excel_file(self, file_path: Path) -> List[Tuple[str, str]]:
        """
        –ü–∞—Ä—Å–∏—Ç Excel —Ñ–∞–π–ª —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏ (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç: –∫–æ–ª–æ–Ω–∫–∞ A - –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ—Ä–º–∏–Ω, –∫–æ–ª–æ–Ω–∫–∞ B - –ø–µ—Ä–µ–≤–æ–¥)
        """
        if not EXCEL_AVAILABLE:
            raise ImportError("openpyxl –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openpyxl")
        
        terms = []
        workbook = openpyxl.load_workbook(file_path)
        sheet = workbook.active
        
        for row in sheet.iter_rows(min_row=2, values_only=True):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if row[0] and row[1]:  # –ï—Å–ª–∏ –æ–±–µ —è—á–µ–π–∫–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã
                source_term = str(row[0]).strip()
                target_term = str(row[1]).strip()
                
                if source_term and target_term:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É —Ç–µ—Ä–º–∏–Ω–æ–≤
                    source_term = self._normalize_text(source_term)
                    target_term = self._normalize_text(target_term)
                    terms.append((source_term, target_term))
        
        return terms
    
    def _normalize_text(self, text: str) -> str:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç, –∏—Å–ø—Ä–∞–≤–ª—è—è –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
        """
        if not text:
            return text
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (—Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏)
            # –°–∏–º–≤–æ–ª—ã —Ç–∏–ø–∞ …ç, …ì, …¨, …® –∏ —Ç.–¥. - —ç—Ç–æ –æ–±—ã—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ UTF-8
            has_suspicious = False
            for char in text:
                code = ord(char)
                # –°–∏–º–≤–æ–ª—ã –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 400-600, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π –∏–ª–∏ –ª–∞—Ç–∏–Ω–∏—Ü–µ–π
                # –≠—Ç–æ —á–∞—Å—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏
                if (400 <= code <= 600 and 
                    not (1040 <= code <= 1103) and  # –Ω–µ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
                    not (65 <= code <= 90) and      # –Ω–µ –ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤–µ—Ä—Ö–Ω–∏–π
                    not (97 <= code <= 122)):       # –Ω–µ –ª–∞—Ç–∏–Ω–∏—Ü–∞ –Ω–∏–∂–Ω–∏–π
                    has_suspicious = True
                    break
            
            # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω—ã –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –ø—Ä–æ–±—É–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç—å
            if has_suspicious:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                for encoding in ['cp1251', 'cp866', 'iso-8859-5']:
                    try:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ latin1 (—á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –±–∞–π—Ç—ã) –∏ –∑–∞—Ç–µ–º –¥–µ–∫–æ–¥–∏—Ä—É–µ–º
                        text_bytes = text.encode('latin1', errors='ignore')
                        decoded = text_bytes.decode(encoding, errors='ignore')
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ª—É—á—à–µ (–º–µ–Ω—å—à–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤)
                        suspicious_count = sum(1 for c in decoded if 400 <= ord(c) <= 600 and 
                                             not (1040 <= ord(c) <= 1103) and
                                             not (65 <= ord(c) <= 90) and
                                             not (97 <= ord(c) <= 122))
                        original_suspicious = sum(1 for c in text if 400 <= ord(c) <= 600 and 
                                                not (1040 <= ord(c) <= 1103) and
                                                not (65 <= ord(c) <= 90) and
                                                not (97 <= ord(c) <= 122))
                        if suspicious_count < original_suspicious:
                            text = decoded
                            break
                    except:
                        continue
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç–µ–∫—Å—Ç –≤ UTF-8
            text = text.encode('utf-8', errors='ignore').decode('utf-8')
            
        except Exception as e:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–∏—Ç—å, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            pass
        
        return text
    
    def _has_valid_text(self, text: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ —Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–Ω–µ —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏)
        """
        if not text:
            return False
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–∫–∏—Ä–∏–ª–ª–∏—Ü–∞, –ª–∞—Ç–∏–Ω–∏—Ü–∞, –∞—Ä–∞–±—Å–∫–∏–µ, –∫–∏—Ç–∞–π—Å–∫–∏–µ, —Ü–∏—Ñ—Ä—ã, –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
        valid_count = 0
        suspicious_count = 0
        
        for char in text:
            code = ord(char)
            # –í–∞–ª–∏–¥–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            if (1040 <= code <= 1103 or  # –∫–∏—Ä–∏–ª–ª–∏—Ü–∞
                65 <= code <= 90 or      # –ª–∞—Ç–∏–Ω–∏—Ü–∞ –≤–µ—Ä—Ö–Ω–∏–π
                97 <= code <= 122 or     # –ª–∞—Ç–∏–Ω–∏—Ü–∞ –Ω–∏–∂–Ω–∏–π
                48 <= code <= 57 or       # —Ü–∏—Ñ—Ä—ã
                0x0600 <= code <= 0x06FF or  # –∞—Ä–∞–±—Å–∫–∏–π
                0x4E00 <= code <= 0x9FFF or  # –∫–∏—Ç–∞–π—Å–∫–∏–π
                char in ".,;:!?()[]{}\"'/-‚Äì‚Äî=+*&%$#@ "):  # –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–æ–±–µ–ª—ã
                valid_count += 1
            # –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (—Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏)
            elif 400 <= code <= 600:
                suspicious_count += 1
        
        # –ï—Å–ª–∏ –±–æ–ª—å—à–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, —á–µ–º –≤–∞–ª–∏–¥–Ω—ã—Ö, —Ç–µ–∫—Å—Ç —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –∏—Å–ø–æ—Ä—á–µ–Ω
        if suspicious_count > valid_count and suspicious_count > 2:
            return False
        
        # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –≤–∞–ª–∏–¥–Ω—ã–π —Å–∏–º–≤–æ–ª
        return valid_count > 0
    
    def parse_file(self, file_path: Path) -> List[Tuple[str, str]]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Ñ–∞–π–ª–∞ –∏ –ø–∞—Ä—Å–∏—Ç –µ–≥–æ
        """
        extension = file_path.suffix.lower()
        
        if extension == ".txt":
            return self.parse_txt_file(file_path)
        elif extension == ".pdf":
            return self.parse_pdf_file(file_path)
        elif extension == ".docx":
            return self.parse_docx_file(file_path)
        elif extension in [".xlsx", ".xls"]:
            return self.parse_excel_file(file_path)
        else:
            raise ValueError(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {extension}")
    
    def build_glossary_dict(
        self, 
        source_lang: str, 
        target_lang: str = "en"
    ) -> Dict[str, Dict[str, str]]:
        """
        –°—Ç—Ä–æ–∏—Ç —Å–ª–æ–≤–∞—Ä—å –≥–ª–æ—Å—Å–∞—Ä–∏—è –∏–∑ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ —è–∑—ã–∫–∞
        
        Returns:
            Dict –≤ —Ñ–æ—Ä–º–∞—Ç–µ: {
                "source_term": {
                    "source": "–∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ—Ä–º–∏–Ω",
                    "target": "–ø–µ—Ä–µ–≤–æ–¥",
                    "abbreviation": "–∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞ –µ—Å–ª–∏ –µ—Å—Ç—å"
                }
            }
        """
        glossary = {}
        lang_dir = self.glossary_dir / source_lang
        
        if not lang_dir.exists():
            print(f"‚ö†Ô∏è  –ü–∞–ø–∫–∞ {lang_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return glossary
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫–µ
        for file_path in lang_dir.iterdir():
            if file_path.is_file():
                try:
                    print(f"üìÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file_path.name}")
                    terms = self.parse_file(file_path)
                    
                    for source_term, target_term in terms:
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –∏–∑ —Å–∫–æ–±–æ–∫
                        source_abbr = None
                        target_abbr = None
                        
                        # –ò—â–µ–º –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã –≤ —Å–∫–æ–±–∫–∞—Ö
                        source_match = re.search(r'\(([^)]+)\)', source_term)
                        if source_match:
                            source_abbr = source_match.group(1)
                            source_term = re.sub(r'\s*\([^)]+\)', '', source_term).strip()
                        
                        target_match = re.search(r'\(([^)]+)\)', target_term)
                        if target_match:
                            target_abbr = target_match.group(1)
                            target_term = re.sub(r'\s*\([^)]+\)', '', target_term).strip()
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ç–µ—Ä–º–∏–Ω—ã –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ç–æ–ª—å–∫–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                        # (—Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–∏)
                        if self._has_valid_text(source_term) and self._has_valid_text(target_term):
                            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ—Ä–º–∏–Ω –∫–∞–∫ –∫–ª—é—á (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ –¥–ª—è –ø–æ–∏—Å–∫–∞)
                            key = source_term.lower()
                            
                            glossary[key] = {
                                "source": source_term,
                                "target": target_term,
                                "source_abbr": source_abbr,
                                "target_abbr": target_abbr
                            }
                        # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ (–Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–π)
                    
                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(terms)} —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ {file_path.name}")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {file_path.name}: {str(e)}")
                    continue
        
        return glossary
    
    def save_glossary_json(
        self, 
        source_lang: str, 
        output_path: Optional[Path] = None,
        target_lang: str = "en"
    ) -> Path:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≥–ª–æ—Å—Å–∞—Ä–∏–π –≤ JSON —Ñ–∞–π–ª
        """
        glossary = self.build_glossary_dict(source_lang, target_lang)
        
        if output_path is None:
            output_path = self.glossary_dir / f"glossary_{source_lang}_to_{target_lang}.json"
        
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(glossary, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ –ì–ª–æ—Å—Å–∞—Ä–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_path}")
        print(f"üìä –í—Å–µ–≥–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(glossary)}")
        
        return output_path
    
    def load_glossary_json(self, json_path: Path) -> Dict[str, Dict[str, str]]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –≥–ª–æ—Å—Å–∞—Ä–∏–π –∏–∑ JSON —Ñ–∞–π–ª–∞
        """
        with open(json_path, "r", encoding="utf-8") as f:
            return json.load(f)


def build_all_glossaries():
    """
    –°–æ–∑–¥–∞–µ—Ç JSON —Ñ–∞–π–ª—ã –¥–ª—è –≤—Å–µ—Ö —è–∑—ã–∫–æ–≤ –≥–ª–æ—Å—Å–∞—Ä–∏—è
    """
    parser = GlossaryParser()
    
    languages = ["russian", "arabic", "chinise"]
    
    for lang in languages:
        print(f"\n{'='*50}")
        print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≥–ª–æ—Å—Å–∞—Ä–∏—è –¥–ª—è —è–∑—ã–∫–∞: {lang}")
        print(f"{'='*50}")
        
        try:
            parser.save_glossary_json(lang)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {lang}: {str(e)}")


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Å–µ—Ö –≥–ª–æ—Å—Å–∞—Ä–∏–µ–≤
    build_all_glossaries()

